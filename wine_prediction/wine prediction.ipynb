{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "072b2cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "#\n",
    "from sklearn.preprocessing import PolynomialFeatures,StandardScaler,OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression,Ridge,Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "#\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential # model\n",
    "from tensorflow.keras.layers import Dense # type of layer\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid,softmax #activation function\n",
    "from tensorflow.keras import regularizers\n",
    "#\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5b64551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>14.23</th>\n",
       "      <th>1.71</th>\n",
       "      <th>2.43</th>\n",
       "      <th>15.6</th>\n",
       "      <th>127</th>\n",
       "      <th>2.8</th>\n",
       "      <th>3.06</th>\n",
       "      <th>0.28</th>\n",
       "      <th>2.29</th>\n",
       "      <th>5.64</th>\n",
       "      <th>1.04</th>\n",
       "      <th>3.92</th>\n",
       "      <th>1065</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>14.20</td>\n",
       "      <td>1.76</td>\n",
       "      <td>2.45</td>\n",
       "      <td>15.2</td>\n",
       "      <td>112</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.97</td>\n",
       "      <td>6.75</td>\n",
       "      <td>1.05</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  14.23  1.71  2.43  15.6  127   2.8  3.06  0.28  2.29  5.64  1.04  3.92  \\\n",
       "0  1  13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28  4.38  1.05  3.40   \n",
       "1  1  13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81  5.68  1.03  3.17   \n",
       "2  1  14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18  7.80  0.86  3.45   \n",
       "3  1  13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82  4.32  1.04  2.93   \n",
       "4  1  14.20  1.76  2.45  15.2  112  3.27  3.39  0.34  1.97  6.75  1.05  2.85   \n",
       "\n",
       "   1065  \n",
       "0  1050  \n",
       "1  1185  \n",
       "2  1480  \n",
       "3   735  \n",
       "4  1450  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:/Users/deban/Desktop/solo/wine/wine.data')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d67602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_names = ['Id',\n",
    "\t'Alcohol',\n",
    " \t'Malic acid',\n",
    " \t'Ash',\n",
    "\t'Alcalinity of ash', \n",
    " \t'Magnesium',\n",
    "\t'Total phenols',\n",
    " \t'Flavanoids',\n",
    " \t'Nonflavanoid phenols',\n",
    " \t'Proanthocyanins',\n",
    "\t'Color intensity',\n",
    " \t'Hue',\n",
    " \t'OD280/OD315 of diluted wines',\n",
    " \t'Proline' ]  \n",
    "df.columns = header_names\n",
    "\n",
    "# Save the DataFrame back to a CSV file\n",
    "df.to_csv('C:/Users/deban/Desktop/solo/wine/wine.data', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e71985a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.736379</td>\n",
       "      <td>2.015862</td>\n",
       "      <td>2.456034</td>\n",
       "      <td>17.062069</td>\n",
       "      <td>105.982759</td>\n",
       "      <td>2.840862</td>\n",
       "      <td>2.981034</td>\n",
       "      <td>0.290172</td>\n",
       "      <td>1.892586</td>\n",
       "      <td>5.526379</td>\n",
       "      <td>1.062414</td>\n",
       "      <td>3.144655</td>\n",
       "      <td>1116.586207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.278732</td>\n",
       "      <td>1.932676</td>\n",
       "      <td>2.244789</td>\n",
       "      <td>20.238028</td>\n",
       "      <td>94.549296</td>\n",
       "      <td>2.258873</td>\n",
       "      <td>2.080845</td>\n",
       "      <td>0.363662</td>\n",
       "      <td>1.630282</td>\n",
       "      <td>3.086620</td>\n",
       "      <td>1.056282</td>\n",
       "      <td>2.785352</td>\n",
       "      <td>519.507042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.153750</td>\n",
       "      <td>3.333750</td>\n",
       "      <td>2.437083</td>\n",
       "      <td>21.416667</td>\n",
       "      <td>99.312500</td>\n",
       "      <td>1.678750</td>\n",
       "      <td>0.781458</td>\n",
       "      <td>0.447500</td>\n",
       "      <td>1.153542</td>\n",
       "      <td>7.396250</td>\n",
       "      <td>0.682708</td>\n",
       "      <td>1.683542</td>\n",
       "      <td>629.895833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Alcohol  Malic acid       Ash  Alcalinity of ash   Magnesium  \\\n",
       "Id                                                                   \n",
       "1   13.736379    2.015862  2.456034          17.062069  105.982759   \n",
       "2   12.278732    1.932676  2.244789          20.238028   94.549296   \n",
       "3   13.153750    3.333750  2.437083          21.416667   99.312500   \n",
       "\n",
       "    Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "Id                                                                     \n",
       "1        2.840862    2.981034              0.290172         1.892586   \n",
       "2        2.258873    2.080845              0.363662         1.630282   \n",
       "3        1.678750    0.781458              0.447500         1.153542   \n",
       "\n",
       "    Color intensity       Hue  OD280/OD315 of diluted wines      Proline  \n",
       "Id                                                                        \n",
       "1          5.526379  1.062414                      3.144655  1116.586207  \n",
       "2          3.086620  1.056282                      2.785352   519.507042  \n",
       "3          7.396250  0.682708                      1.683542   629.895833  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Id').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "09728130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    71\n",
       "1    58\n",
       "3    48\n",
       "Name: Id, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d0122928",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace({'Id':{1:0, 2:1, 3:2}}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "061f29de",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.drop(['Id'],axis=1).values\n",
    "y=df['Id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "419356ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.320e+01 1.780e+00 2.140e+00 1.120e+01 1.000e+02 2.650e+00 2.760e+00\n",
      "  2.600e-01 1.280e+00 4.380e+00 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 1.860e+01 1.010e+02 2.800e+00 3.240e+00\n",
      "  3.000e-01 2.810e+00 5.680e+00 1.030e+00 3.170e+00 1.185e+03]\n",
      " [1.437e+01 1.950e+00 2.500e+00 1.680e+01 1.130e+02 3.850e+00 3.490e+00\n",
      "  2.400e-01 2.180e+00 7.800e+00 8.600e-01 3.450e+00 1.480e+03]\n",
      " [1.324e+01 2.590e+00 2.870e+00 2.100e+01 1.180e+02 2.800e+00 2.690e+00\n",
      "  3.900e-01 1.820e+00 4.320e+00 1.040e+00 2.930e+00 7.350e+02]\n",
      " [1.420e+01 1.760e+00 2.450e+00 1.520e+01 1.120e+02 3.270e+00 3.390e+00\n",
      "  3.400e-01 1.970e+00 6.750e+00 1.050e+00 2.850e+00 1.450e+03]]\n"
     ]
    }
   ],
   "source": [
    "std=StandardScaler()\n",
    "x_std=std.fit_transform(x)\n",
    "print(x[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f9f87e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x_std,y,train_size=0.9,random_state=1,stratify=y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2a423c9",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "af959bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m,n=x_train.shape\n",
    "lamb=0.001\n",
    "l_r=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "18a71a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural network\n",
    "tf.random.set_seed(1234) # for consistent results\n",
    "model_NN = Sequential(\n",
    "    [                \n",
    "        tf.keras.layers.InputLayer((n,)),\n",
    "        tf.keras.layers.Dense(13, activation=\"relu\",kernel_regularizer=tf.keras.regularizers.l2(lamb), name=\"L1\"),\n",
    "        tf.keras.layers.Dense(7, activation=\"relu\",kernel_regularizer=tf.keras.regularizers.l2(lamb), name=\"L2\"),\n",
    "        tf.keras.layers.Dense(3, activation=\"linear\", name=\"L3\")\n",
    "    ], name = \"my_model\" \n",
    ")\n",
    "model_NN.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(l_r)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1aabb7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0242\n",
      "Epoch 2/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 899us/step - loss: 0.0242\n",
      "Epoch 3/400\n",
      "5/5 [==============================] - 0s 998us/step - loss: 0.0241\n",
      "Epoch 4/400\n",
      "5/5 [==============================] - 0s 925us/step - loss: 0.0241\n",
      "Epoch 5/400\n",
      "5/5 [==============================] - 0s 999us/step - loss: 0.0241\n",
      "Epoch 6/400\n",
      "5/5 [==============================] - 0s 913us/step - loss: 0.0240\n",
      "Epoch 7/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0240\n",
      "Epoch 8/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0239\n",
      "Epoch 9/400\n",
      "5/5 [==============================] - 0s 852us/step - loss: 0.0239\n",
      "Epoch 10/400\n",
      "5/5 [==============================] - 0s 879us/step - loss: 0.0239\n",
      "Epoch 11/400\n",
      "5/5 [==============================] - 0s 977us/step - loss: 0.0238\n",
      "Epoch 12/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0238\n",
      "Epoch 13/400\n",
      "5/5 [==============================] - 0s 874us/step - loss: 0.0237\n",
      "Epoch 14/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0237\n",
      "Epoch 15/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0237\n",
      "Epoch 16/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0236\n",
      "Epoch 17/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0236\n",
      "Epoch 18/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0235\n",
      "Epoch 19/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0235\n",
      "Epoch 20/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0235\n",
      "Epoch 21/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0234\n",
      "Epoch 22/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0234\n",
      "Epoch 23/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0233\n",
      "Epoch 24/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0233\n",
      "Epoch 25/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0233\n",
      "Epoch 26/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0232\n",
      "Epoch 27/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0232\n",
      "Epoch 28/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0232\n",
      "Epoch 29/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0231\n",
      "Epoch 30/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0231\n",
      "Epoch 31/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0230\n",
      "Epoch 32/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0230\n",
      "Epoch 33/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0230\n",
      "Epoch 34/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0229\n",
      "Epoch 35/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0229\n",
      "Epoch 36/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0228\n",
      "Epoch 37/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0228\n",
      "Epoch 38/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0228\n",
      "Epoch 39/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0227\n",
      "Epoch 40/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0227\n",
      "Epoch 41/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0226\n",
      "Epoch 42/400\n",
      "5/5 [==============================] - 0s 899us/step - loss: 0.0226\n",
      "Epoch 43/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0226\n",
      "Epoch 44/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0225\n",
      "Epoch 45/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0225\n",
      "Epoch 46/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0225\n",
      "Epoch 47/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0224\n",
      "Epoch 48/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0224\n",
      "Epoch 49/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0223\n",
      "Epoch 50/400\n",
      "5/5 [==============================] - 0s 946us/step - loss: 0.0223\n",
      "Epoch 51/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0223\n",
      "Epoch 52/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0222\n",
      "Epoch 53/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0222\n",
      "Epoch 54/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0222\n",
      "Epoch 55/400\n",
      "5/5 [==============================] - 0s 904us/step - loss: 0.0221\n",
      "Epoch 56/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0221\n",
      "Epoch 57/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0220\n",
      "Epoch 58/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0220\n",
      "Epoch 59/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0220\n",
      "Epoch 60/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0219\n",
      "Epoch 61/400\n",
      "5/5 [==============================] - 0s 767us/step - loss: 0.0219\n",
      "Epoch 62/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0219\n",
      "Epoch 63/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0218\n",
      "Epoch 64/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0218\n",
      "Epoch 65/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0217\n",
      "Epoch 66/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0217\n",
      "Epoch 67/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0217\n",
      "Epoch 68/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0216\n",
      "Epoch 69/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0216\n",
      "Epoch 70/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0216\n",
      "Epoch 71/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0215\n",
      "Epoch 72/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0215\n",
      "Epoch 73/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0215\n",
      "Epoch 74/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0214\n",
      "Epoch 75/400\n",
      "5/5 [==============================] - 0s 918us/step - loss: 0.0214\n",
      "Epoch 76/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0213\n",
      "Epoch 77/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0213\n",
      "Epoch 78/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0213\n",
      "Epoch 79/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0212\n",
      "Epoch 80/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0212\n",
      "Epoch 81/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0212\n",
      "Epoch 82/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0211\n",
      "Epoch 83/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0211\n",
      "Epoch 84/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0210\n",
      "Epoch 85/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0210\n",
      "Epoch 86/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0210\n",
      "Epoch 87/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0209\n",
      "Epoch 88/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0209\n",
      "Epoch 89/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0209\n",
      "Epoch 90/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0208\n",
      "Epoch 91/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0208\n",
      "Epoch 92/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0208\n",
      "Epoch 93/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0207\n",
      "Epoch 94/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0207\n",
      "Epoch 95/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0206\n",
      "Epoch 96/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0206\n",
      "Epoch 97/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0206\n",
      "Epoch 98/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0205\n",
      "Epoch 99/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0205\n",
      "Epoch 100/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0205\n",
      "Epoch 101/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0204\n",
      "Epoch 102/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0204\n",
      "Epoch 103/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0204\n",
      "Epoch 104/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0203\n",
      "Epoch 105/400\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0203\n",
      "Epoch 106/400\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0203\n",
      "Epoch 107/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0202\n",
      "Epoch 108/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0202\n",
      "Epoch 109/400\n",
      "5/5 [==============================] - 0s 853us/step - loss: 0.0202\n",
      "Epoch 110/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0201\n",
      "Epoch 111/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0201\n",
      "Epoch 112/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0200\n",
      "Epoch 113/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0200\n",
      "Epoch 114/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0200\n",
      "Epoch 115/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0199\n",
      "Epoch 116/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0199\n",
      "Epoch 117/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0199\n",
      "Epoch 118/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0198\n",
      "Epoch 119/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0198\n",
      "Epoch 120/400\n",
      "5/5 [==============================] - 0s 923us/step - loss: 0.0198\n",
      "Epoch 121/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0197\n",
      "Epoch 122/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0197\n",
      "Epoch 123/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0197\n",
      "Epoch 124/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0196\n",
      "Epoch 125/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0196\n",
      "Epoch 126/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0196\n",
      "Epoch 127/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0195\n",
      "Epoch 128/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0195\n",
      "Epoch 129/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0195\n",
      "Epoch 130/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0194\n",
      "Epoch 131/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0194\n",
      "Epoch 132/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0194\n",
      "Epoch 133/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0193\n",
      "Epoch 134/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0193\n",
      "Epoch 135/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0193\n",
      "Epoch 136/400\n",
      "5/5 [==============================] - 0s 909us/step - loss: 0.0192\n",
      "Epoch 137/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0192\n",
      "Epoch 138/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0192\n",
      "Epoch 139/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0191\n",
      "Epoch 140/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0191\n",
      "Epoch 141/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0191\n",
      "Epoch 142/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0190\n",
      "Epoch 143/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0190\n",
      "Epoch 144/400\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0190\n",
      "Epoch 145/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0189\n",
      "Epoch 146/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0189\n",
      "Epoch 147/400\n",
      "5/5 [==============================] - 0s 904us/step - loss: 0.0189\n",
      "Epoch 148/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0188\n",
      "Epoch 149/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0188\n",
      "Epoch 150/400\n",
      "5/5 [==============================] - 0s 1000us/step - loss: 0.0188\n",
      "Epoch 151/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0187\n",
      "Epoch 152/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0187\n",
      "Epoch 153/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0187\n",
      "Epoch 154/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0186\n",
      "Epoch 155/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0186\n",
      "Epoch 156/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0186\n",
      "Epoch 157/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0186\n",
      "Epoch 158/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0185\n",
      "Epoch 159/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0185\n",
      "Epoch 160/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0185\n",
      "Epoch 161/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0184\n",
      "Epoch 162/400\n",
      "5/5 [==============================] - 0s 775us/step - loss: 0.0184\n",
      "Epoch 163/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0184\n",
      "Epoch 164/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0183\n",
      "Epoch 165/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0183\n",
      "Epoch 166/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0183\n",
      "Epoch 167/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0182\n",
      "Epoch 168/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0182\n",
      "Epoch 169/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0182\n",
      "Epoch 170/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0181\n",
      "Epoch 171/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0181\n",
      "Epoch 172/400\n",
      "5/5 [==============================] - 0s 988us/step - loss: 0.0181\n",
      "Epoch 173/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0181\n",
      "Epoch 174/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0180\n",
      "Epoch 175/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0180\n",
      "Epoch 176/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0180\n",
      "Epoch 177/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0179\n",
      "Epoch 178/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0179\n",
      "Epoch 179/400\n",
      "5/5 [==============================] - 0s 884us/step - loss: 0.0179\n",
      "Epoch 180/400\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0178\n",
      "Epoch 181/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0178\n",
      "Epoch 182/400\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0178\n",
      "Epoch 183/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0178\n",
      "Epoch 184/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0177\n",
      "Epoch 185/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0177\n",
      "Epoch 186/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0177\n",
      "Epoch 187/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0176\n",
      "Epoch 188/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0176\n",
      "Epoch 189/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0176\n",
      "Epoch 190/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0176\n",
      "Epoch 191/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0175\n",
      "Epoch 192/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0175\n",
      "Epoch 193/400\n",
      "5/5 [==============================] - 0s 896us/step - loss: 0.0175\n",
      "Epoch 194/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0174\n",
      "Epoch 195/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0174\n",
      "Epoch 196/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0174\n",
      "Epoch 197/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0174\n",
      "Epoch 198/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0173\n",
      "Epoch 199/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0173\n",
      "Epoch 200/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0173\n",
      "Epoch 201/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0172\n",
      "Epoch 202/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0172\n",
      "Epoch 203/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0172\n",
      "Epoch 204/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0172\n",
      "Epoch 205/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0171\n",
      "Epoch 206/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0171\n",
      "Epoch 207/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0171\n",
      "Epoch 208/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0170\n",
      "Epoch 209/400\n",
      "5/5 [==============================] - 0s 875us/step - loss: 0.0170\n",
      "Epoch 210/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0170\n",
      "Epoch 211/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0170\n",
      "Epoch 212/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0169\n",
      "Epoch 213/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0169\n",
      "Epoch 214/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0169\n",
      "Epoch 215/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0168\n",
      "Epoch 216/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0168\n",
      "Epoch 217/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0168\n",
      "Epoch 218/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0168\n",
      "Epoch 219/400\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0167\n",
      "Epoch 220/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0167\n",
      "Epoch 221/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0167\n",
      "Epoch 222/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0167\n",
      "Epoch 223/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0166\n",
      "Epoch 224/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0166\n",
      "Epoch 225/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0166\n",
      "Epoch 226/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0165\n",
      "Epoch 227/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0165\n",
      "Epoch 228/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0165\n",
      "Epoch 229/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0165\n",
      "Epoch 230/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0164\n",
      "Epoch 231/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0164\n",
      "Epoch 232/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0164\n",
      "Epoch 233/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0164\n",
      "Epoch 234/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0163\n",
      "Epoch 235/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0163\n",
      "Epoch 236/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0163\n",
      "Epoch 237/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0163\n",
      "Epoch 238/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0162\n",
      "Epoch 239/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0162\n",
      "Epoch 240/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0162\n",
      "Epoch 241/400\n",
      "5/5 [==============================] - 0s 871us/step - loss: 0.0162\n",
      "Epoch 242/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0161\n",
      "Epoch 243/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0161\n",
      "Epoch 244/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0161\n",
      "Epoch 245/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0160\n",
      "Epoch 246/400\n",
      "5/5 [==============================] - 0s 848us/step - loss: 0.0160\n",
      "Epoch 247/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0160\n",
      "Epoch 248/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0160\n",
      "Epoch 249/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0159\n",
      "Epoch 250/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0159\n",
      "Epoch 251/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0159\n",
      "Epoch 252/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0159\n",
      "Epoch 253/400\n",
      "5/5 [==============================] - 0s 835us/step - loss: 0.0158\n",
      "Epoch 254/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0158\n",
      "Epoch 255/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0158\n",
      "Epoch 256/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0158\n",
      "Epoch 257/400\n",
      "5/5 [==============================] - 0s 855us/step - loss: 0.0157\n",
      "Epoch 258/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0157\n",
      "Epoch 259/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0157\n",
      "Epoch 260/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0157\n",
      "Epoch 261/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0157\n",
      "Epoch 262/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0156\n",
      "Epoch 263/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0156\n",
      "Epoch 264/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0156\n",
      "Epoch 265/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0155\n",
      "Epoch 266/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0155\n",
      "Epoch 267/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0155\n",
      "Epoch 268/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0155\n",
      "Epoch 269/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0155\n",
      "Epoch 270/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0154\n",
      "Epoch 271/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0154\n",
      "Epoch 272/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0154\n",
      "Epoch 273/400\n",
      "5/5 [==============================] - 0s 979us/step - loss: 0.0154\n",
      "Epoch 274/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0153\n",
      "Epoch 275/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0153\n",
      "Epoch 276/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0153\n",
      "Epoch 277/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0153\n",
      "Epoch 278/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0152\n",
      "Epoch 279/400\n",
      "5/5 [==============================] - 0s 996us/step - loss: 0.0152\n",
      "Epoch 280/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0152\n",
      "Epoch 281/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0152\n",
      "Epoch 282/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0151\n",
      "Epoch 283/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0151\n",
      "Epoch 284/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0151\n",
      "Epoch 285/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0151\n",
      "Epoch 286/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0151\n",
      "Epoch 287/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0150\n",
      "Epoch 288/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0150\n",
      "Epoch 289/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0150\n",
      "Epoch 290/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0150\n",
      "Epoch 291/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0149\n",
      "Epoch 292/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0149\n",
      "Epoch 293/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0149\n",
      "Epoch 294/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0149\n",
      "Epoch 295/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0148\n",
      "Epoch 296/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0148\n",
      "Epoch 297/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0148\n",
      "Epoch 298/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0148\n",
      "Epoch 299/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0148\n",
      "Epoch 300/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0147\n",
      "Epoch 301/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0147\n",
      "Epoch 302/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0147\n",
      "Epoch 303/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0147\n",
      "Epoch 304/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0146\n",
      "Epoch 305/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0146\n",
      "Epoch 306/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0146\n",
      "Epoch 307/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0146\n",
      "Epoch 308/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0146\n",
      "Epoch 309/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0145\n",
      "Epoch 310/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0145\n",
      "Epoch 311/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0145\n",
      "Epoch 312/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0145\n",
      "Epoch 313/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0144\n",
      "Epoch 314/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0144\n",
      "Epoch 315/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0144\n",
      "Epoch 316/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0144\n",
      "Epoch 317/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0144\n",
      "Epoch 318/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0143\n",
      "Epoch 319/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0143\n",
      "Epoch 320/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0143\n",
      "Epoch 321/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0143\n",
      "Epoch 322/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0142\n",
      "Epoch 323/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0142\n",
      "Epoch 324/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0142\n",
      "Epoch 325/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0142\n",
      "Epoch 326/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0141\n",
      "Epoch 327/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0141\n",
      "Epoch 328/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0141\n",
      "Epoch 329/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0141\n",
      "Epoch 330/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0141\n",
      "Epoch 331/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0140\n",
      "Epoch 332/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0140\n",
      "Epoch 333/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0140\n",
      "Epoch 334/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0140\n",
      "Epoch 335/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0139\n",
      "Epoch 336/400\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0139\n",
      "Epoch 337/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0139\n",
      "Epoch 338/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0139\n",
      "Epoch 339/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0139\n",
      "Epoch 340/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0138\n",
      "Epoch 341/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0138\n",
      "Epoch 342/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0138\n",
      "Epoch 343/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0138\n",
      "Epoch 344/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0138\n",
      "Epoch 345/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0137\n",
      "Epoch 346/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0137\n",
      "Epoch 347/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0137\n",
      "Epoch 348/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0137\n",
      "Epoch 349/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0137\n",
      "Epoch 350/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0136\n",
      "Epoch 351/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0136\n",
      "Epoch 352/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0136\n",
      "Epoch 353/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0136\n",
      "Epoch 354/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0136\n",
      "Epoch 355/400\n",
      "5/5 [==============================] - 0s 750us/step - loss: 0.0135\n",
      "Epoch 356/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0135\n",
      "Epoch 357/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0135\n",
      "Epoch 358/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0135\n",
      "Epoch 359/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0135\n",
      "Epoch 360/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0134\n",
      "Epoch 361/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0134\n",
      "Epoch 362/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0134\n",
      "Epoch 363/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0134\n",
      "Epoch 364/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0134\n",
      "Epoch 365/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0133\n",
      "Epoch 366/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0133\n",
      "Epoch 367/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0133\n",
      "Epoch 368/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0133\n",
      "Epoch 369/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0133\n",
      "Epoch 370/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0132\n",
      "Epoch 371/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0132\n",
      "Epoch 372/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0132\n",
      "Epoch 373/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0132\n",
      "Epoch 374/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0132\n",
      "Epoch 375/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0131\n",
      "Epoch 376/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0131\n",
      "Epoch 377/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0131\n",
      "Epoch 378/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0131\n",
      "Epoch 379/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0131\n",
      "Epoch 380/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0131\n",
      "Epoch 381/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0130\n",
      "Epoch 382/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0130\n",
      "Epoch 383/400\n",
      "5/5 [==============================] - 0s 938us/step - loss: 0.0130\n",
      "Epoch 384/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0130\n",
      "Epoch 385/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0130\n",
      "Epoch 386/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0129\n",
      "Epoch 387/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0129\n",
      "Epoch 388/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0129\n",
      "Epoch 389/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0129\n",
      "Epoch 390/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0129\n",
      "Epoch 391/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0129\n",
      "Epoch 392/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0128\n",
      "Epoch 393/400\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0128\n",
      "Epoch 394/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0128\n",
      "Epoch 395/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0128\n",
      "Epoch 396/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0128\n",
      "Epoch 397/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0128\n",
      "Epoch 398/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0127\n",
      "Epoch 399/400\n",
      "5/5 [==============================] - 0s 881us/step - loss: 0.0127\n",
      "Epoch 400/400\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2560e009d90>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_NN.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d69ee7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 765us/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        52\n",
      "           1       1.00      1.00      1.00        64\n",
      "           2       1.00      1.00      1.00        43\n",
      "\n",
      "    accuracy                           1.00       159\n",
      "   macro avg       1.00      1.00      1.00       159\n",
      "weighted avg       1.00      1.00      1.00       159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = tf.nn.softmax(model_NN.predict(x_train))\n",
    "y_pred_labels_train = np.argmax(y_pred, axis=1)\n",
    "report = classification_report(y_train, y_pred_labels_train)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "97608d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         6\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        18\n",
      "   macro avg       1.00      1.00      1.00        18\n",
      "weighted avg       1.00      1.00      1.00        18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = tf.nn.softmax(model_NN.predict(x_test))\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "report = classification_report(y_test, y_pred_labels)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "53ca3ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective='multi:softprob', predictor=None, ...)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#XGBooster\n",
    "model_XG=XGBClassifier()\n",
    "model_XG.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bc9cb6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        52\n",
      "           1       1.00      1.00      1.00        64\n",
      "           2       1.00      1.00      1.00        43\n",
      "\n",
      "    accuracy                           1.00       159\n",
      "   macro avg       1.00      1.00      1.00       159\n",
      "weighted avg       1.00      1.00      1.00       159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_labels = model_XG.predict(x_train)\n",
    "report = classification_report(y_train, y_pred_labels)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b629bc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         6\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        18\n",
      "   macro avg       1.00      1.00      1.00        18\n",
      "weighted avg       1.00      1.00      1.00        18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_labels = model_XG.predict(x_test)\n",
    "report = classification_report(y_test, y_pred_labels)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d7509a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
